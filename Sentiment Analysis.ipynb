{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Lots of libraries exist that will do sentiment analysis for you. Imagine that: just taking a sentence, throwing it into a library, and geting back a score! How convenient!\n",
    "\n",
    "It's also **totally irresponsible** unless you know how the sentiment analyzer was built. In this homework we're going to see how sentiment analysis is done with a few packages.\n",
    "\n",
    "## Installation\n",
    "\n",
    "If you haven't already, you'll want to `pip install` two packages: NLTK and Textblob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (3.4.4)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: textblob in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from textblob) (3.4.4)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nltk>=3.1->textblob) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK: Natural Language Tooklit\n",
    "\n",
    "[Natural Language Toolkit](https://www.nltk.org/) is the basis for a lot of text analysis done in Python. It's old and terrible and slow, but it's just been used for so long and does so many things that it's generally the default when people get into text analysis. The new kid on the block is [spaCy](https://spacy.io/) (but it doesn't do sentiment analysis so we're leaving it out of this).\n",
    "\n",
    "When you first run NLTK, you need to download some datasets to make sure it will be able to do everything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading vader_lexicon: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1056)>\n",
      "[nltk_data] Error loading movie_reviews: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1056)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1056)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do sentiment analysis with NLTK, it only takes a couple lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.153, 'neu': 0.688, 'pos': 0.159, 'compound': 0.0276}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "sia = SIA()\n",
    "sia.polarity_scores(\"This restaurant was great, but I'm not sure if I'll go there again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking `SentimentIntensityAnalyzer` for the `polarity_score` gave us four values in a dictionary:\n",
    "\n",
    "- **negative:** the negative sentiment in a sentence\n",
    "- **neutral:** the neutral sentiment in a sentence\n",
    "- **positive:** the postivie sentiment in the sentence\n",
    "- **compound:** the aggregated sentiment. \n",
    "    \n",
    "Seems simple enough!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I just got a call from my boss - does he realise it's Saturday?\"\n",
    "sia.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in real life, if you use an emoji you can be read as being more positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.786, 'pos': 0.214, 'compound': 0.4588}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I just got a call from my boss - does he realise it's Saturday? :)\"\n",
    "sia.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I just got a call from my boss - does he realise it's Saturday? ðŸ˜Š\"\n",
    "sia.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why didn't it understand the emoji the same way it understood the emoticon? Well, **it only knows the words that it's been trained on,** and if VADER's never seen ðŸ˜Š before it won't know what to think of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextBlob\n",
    "\n",
    "TextBlob is built on top of NLTK, but is infinitely easier to use. It's still slow, but _it's so so so easy to use_. \n",
    "\n",
    "You can just feed TextBlob your sentence, then ask for a `.sentiment`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.275, subjectivity=0.8194444444444444)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"This restaurant was great, but I'm not sure if I'll go there again.\")\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How could it possibly be easier than that?!?!?** This time we get a `polarity` and a `subjectivity` instead of all of those different scores, but it's basically the same idea.\n",
    "\n",
    "If you like options: it turns out TextBlob actually has multiple sentiment analysis tools! How fun! We can plug in a different analyzer to get a different result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(classification='pos', p_pos=0.5879425317005774, p_neg=0.41205746829942275)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"This restaurant was great, but I'm not sure if I'll go there again.\", analyzer=NaiveBayesAnalyzer())\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's a **very different result.** To understand why it's so different, we need to talk about where these sentiment numbers come from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But where do those numbers come from?\n",
    "\n",
    "The most important thing to understand is **sentiment is always just an opinion.** In this case it's an opinion, yes, but specifically **the opinion of a machine.**\n",
    "\n",
    "## VADER\n",
    "\n",
    "NLTK's Sentiment Intensity Analyzer works is using something called **VADER**, which is a list of words that have a sentiment associated with each of them.\n",
    "\n",
    "|Word|Sentiment rating|\n",
    "|---|---|\n",
    "|tragedy|-3.4|\n",
    "|rejoiced|2.0|\n",
    "|disaster|-3.1|\n",
    "|great|3.1|\n",
    "\n",
    "If you have more positives, the sentence is more positive. If you have more negatives, it's more negative. It can also take into account things like capitalization - you can read more about the classifier [here](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html), or the actual paper it came out of [here](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf).\n",
    "\n",
    "**How do they know what's positive/negative?** They came up with a very big list of words, then asked people on the internet and paid them one cent for each word they scored.\n",
    "\n",
    "## TextBlob's `.sentiment`\n",
    "\n",
    "TextBlob's sentiment analysis is based on a separate library called [pattern](https://www.clips.uantwerpen.be/pattern).\n",
    "\n",
    "> The sentiment analysis lexicon bundled in Pattern focuses on adjectives. It contains adjectives that occur frequently in customer reviews, hand-tagged with values for polarity and subjectivity.\n",
    "\n",
    "Same kind of thing as NLTK's VADER, but it specifically looks at words from customer reviews.\n",
    "\n",
    "**How do they know what's positive/negative?** They look at (mostly) adjectives that occur in customer reviews and hand-tag them.\n",
    "\n",
    "## TextBlob's `.sentiment` + NaiveBayesAnalyzer\n",
    "\n",
    "TextBlob's other option uses a `NaiveBayesAnalyzer`, which is a machine learning technique. When you use this option with TextBlob, the sentiment is coming from \"an NLTK classifier trained on a movie reviews corpus.\"\n",
    "\n",
    "**How do they know what's positive/negative?** Looked at movie reviews and scores using machine learning, see what words are associated with a positive/negative rating.\n",
    "\n",
    "## What's this mean for me?\n",
    "\n",
    "When you're doing automatic sentiment analysis, you have two major questions: \n",
    "\n",
    "* Where does the list of known words come from\n",
    "* Where do the positive/negative scores come from\n",
    "\n",
    "Let's compare the tools we've used so far.\n",
    "\n",
    "|technique|word source|word selection|scores|\n",
    "|---|---|---|---|\n",
    "|NLTK (VADER)|everywhere|hand-picked|internet people, word-by-word|\n",
    "|TextBlob|product reviews|hand-picked, mostly adjectives|internet people, word-by-word|\n",
    "|TextBlob + NaiveBayesAnalyzer|movie reviews|all words|automatic based on score|\n",
    "\n",
    "A major thing that should jump out at you is **how different the sources are.**\n",
    "\n",
    "While VADER focuses on content found everywhere, TextBlob's two options are specific to certain domains. The [original paper for VADER](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf) passive-aggressively noted that VADER is effective at general use, but being trained on a specific domain can have benefits: \n",
    "\n",
    "> While some algorithms performed decently on test data from the specific domain for which it was expressly trained, they do not significantly outstrip the simple model we use.\n",
    "\n",
    "They're basically saying, \"if you train a model on words from a certain field, it will be good at that field.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "### Question 1: Is it okay to use a sentiment analyzer built on product reviews to check the sentiment of tweets? How about to check the sentiment of wine reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would say it is not the best to use it for the sentiment of tweets, while on the sentiment of wine reviews \n",
    "# it could possibly be more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Is it okay to use a sentiment analyzer trained on everything to check the sentiment of tweets? How about to check the sentiment of wine reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think this would make more sense, although there is the challenge of adapting to Twitters specific code of\n",
    "# language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: If I'm trying to report on whether people generally like or dislike what is happening throughout the Democratic debates, could I use these sorts of tools on tweets? Let's hear arguments for both sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our own sentiment analyzer\n",
    "\n",
    "We don't want to rely on other people, we want to do this ourselves! There are two major ways to do sentiment analysis:\n",
    "\n",
    "* Have a list of words that you humans assign positive or negative scores to\n",
    "* Look at something scored (movie reviews, product reviews) and figure out which words appear with which scores\n",
    "\n",
    "Depending on how you look at it, it's either a classification or a regression problem. We'll see the difference down below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on tweets\n",
    "\n",
    "Let's say we were going to analyze the sentiment of tweets. If we had a list of tweets that were scored positive vs. negative, we could see which words are usually associated with positive scores and which are usually associated with negative scores.\n",
    "\n",
    "Luckily, we have **Sentiment140** - http://help.sentiment140.com/for-students - a list of 1.6 million tweets along with a score as to whether they're negative (0) or positive (4). We'll use it to build our own machine learning algorithm to see separate positivity from negativity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>query</th>\n",
       "      <th>username</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity          id                      datetime     query  \\\n",
       "0         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1         0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2         0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3         0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4         0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "          username  \\\n",
       "0  _TheSpecialOne_   \n",
       "1    scotthamilton   \n",
       "2         mattycus   \n",
       "3          ElleCTF   \n",
       "4           Karoli   \n",
       "\n",
       "                                                                                                               content  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "1      is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!  \n",
       "2                            @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds  \n",
       "3                                                                      my whole body feels itchy and like its on fire   \n",
       "4      @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.   "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = ['polarity', 'id', 'datetime', 'query', 'username', 'content']\n",
    "df = pd.read_csv(\"trainingandtestdata/training.1600000.processed.noemoticon.csv\", \n",
    "                 names=columns,\n",
    "                 encoding='latin-1')\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning our data\n",
    "\n",
    "The `polarity` field is whether something is positive or negative. How many do we have of each?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    800000\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the documentation, `0` is negative and `4` is positive. Weird, right? Let's make it zero and one instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['polarity'] = df['polarity'].astype(str).str.replace(\"4\", \"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm you have 800k of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    800000\n",
       "0    800000\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a **lot of tweets.**\n",
    "\n",
    "Let's be honest: it's going to take our algorithms a long long time to process that many. Instead of working with our entire dataframe, let's use a **sample of 20,000**. If things are still slow before we can decrease this number.\n",
    "\n",
    "* **Tip:** `df.sample(5)` will give you a sample of 5 elements of your dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.sample(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm you have 20,000 rows and 6 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 6)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize our tweets\n",
    "\n",
    "Create a `TfidfVectorizer` and use it to vectorize our tweets. Since we don't have all the time in the world, we should probably use `max_features` to only take a selection of terms - how about 2000 for now?\n",
    "\n",
    "* **Tip:** Your end result should be a `words_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>datetime</th>\n",
       "      <th>id</th>\n",
       "      <th>polarity</th>\n",
       "      <th>query</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   content  datetime   id  polarity  query  username\n",
       "0      0.0       0.0  0.0       1.0    0.0       0.0\n",
       "1      0.0       0.0  1.0       0.0    0.0       0.0\n",
       "2      0.0       1.0  0.0       0.0    0.0       0.0\n",
       "3      0.0       0.0  0.0       0.0    1.0       0.0\n",
       "4      0.0       0.0  0.0       0.0    0.0       1.0\n",
       "5      1.0       0.0  0.0       0.0    0.0       0.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', use_idf=False, norm='l1')\n",
    "X = tfidf_vectorizer.fit_transform(df_new)\n",
    "pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your dataframe should look something like\n",
    "\n",
    "|00|000|10|...|your|...|yummy|yup|Â½t|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|0.0|0.0|0.0|...|0.0|...|0.0|0.0|0.0|\n",
    "|0.0|0.0|0.0|...|0.0|...|0.0|0.0|0.0|\n",
    "|0.0|0.0|0.0|...|0.235754|...|0.0|0.0|0.0|\n",
    "|0.0|0.0|0.0|...|0.0|...|0.0|0.0|0.0|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6x6 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our algorithm\n",
    "\n",
    "### Setting up our variables\n",
    "\n",
    "Create an `X` and a `y`, same as ever. In this case, what are our **features** and what are our **labels?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that `X` has 20,000 rows and 2,000 columns, and that `y` has 20,000 rows of 1 column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking an algorithm\n",
    "\n",
    "What kind of algorithm do we want? We've used quite a few, and I just pulled another one couple classifiers of thin air."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When picking an algorithm, think about what the output should be: is it a category? A probability, an amount? In this case **it might be any of those!**\n",
    "\n",
    "For example:\n",
    "\n",
    "* **A category:** `0` or `1` for negative vs positive\n",
    "* **A probability:** The % chance that it's either negative or positive (between 0 and 1)\n",
    "* **An amount:** A score between 0 and 1 about how positive it is\n",
    "\n",
    "So hey, let's just make **one of each** of these. Name them `linreg`, `logreg`, `forest`, `svc`, and `bayes`.\n",
    "\n",
    "The two new ones - `LinearSVC` and `MultinomialNB` - work exactly the same as your other classifiers, you'll be doing the standard creation and fitting:\n",
    "\n",
    "```python\n",
    "svc = LinearSVC()\n",
    "svc.fit(X, y)\n",
    "```\n",
    "\n",
    "**Create and train classifiers in the cells below.** Add `%%time` to the top of each cell to see how long they take to train.\n",
    "\n",
    "* **Tip:** Remember you need to add `C=1e9` to logistic regression, and specify the solver!\n",
    "* **Tip:** If the logistic regression doesn't converge, it hasn't found an answer. You might need to increase `max_iter` (the default is 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 Âµs, sys: 1 Âµs, total: 6 Âµs\n",
      "Wall time: 11 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create and train a linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 Âµs, sys: 1 Âµs, total: 5 Âµs\n",
      "Wall time: 9.06 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create and train a logistic regression - if it doesn't converge be sure to increase max_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 Âµs, sys: 1 Âµs, total: 5 Âµs\n",
      "Wall time: 9.06 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create and train a random forest classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 Âµs, sys: 1e+03 ns, total: 5 Âµs\n",
      "Wall time: 8.34 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create and train a linear support vector classifier (LinearSVC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 Âµs, sys: 1e+03 ns, total: 5 Âµs\n",
      "Wall time: 8.82 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create and train a multinomial naive bayes classifier (MultinomialNB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How long did each take to train?** How much faster were some compared to others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use our models on some new data\n",
    "\n",
    "Now that we've trained our models, **they can try to predict whether a model is positive or negative**.\n",
    "\n",
    "**Add five more sentences to the list below.** They should be a mix of positive and negative. They can be boring, they can be exciting, they can be short, they can be long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm not sure how I feel about toast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did you see the baseball game yesterday?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The package was delivered late and the contents were broken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trashy television shows are some of my favorites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm seeing a Kubrick film tomorrow, I hear not so great things about it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I find chirping birds irritating, but I know I'm not the only one</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    content\n",
       "0                                       I'm not sure how I feel about toast\n",
       "1                                  Did you see the baseball game yesterday?\n",
       "2               The package was delivered late and the contents were broken\n",
       "3                          Trashy television shows are some of my favorites\n",
       "4  I'm seeing a Kubrick film tomorrow, I hear not so great things about it.\n",
       "5         I find chirping birds irritating, but I know I'm not the only one"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create some test data\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "unknown = pd.DataFrame([\n",
    "       \"I'm not sure how I feel about toast\",\n",
    "       \"Did you see the baseball game yesterday?\",\n",
    "       \"The package was delivered late and the contents were broken\",\n",
    "       \"Trashy television shows are some of my favorites\",\n",
    "       \"I'm seeing a Kubrick film tomorrow, I hear not so great things about it.\",\n",
    "       \"I find chirping birds irritating, but I know I'm not the only one\"\n",
    "], columns=['content'])\n",
    "unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to **vectorizer** our sentences into numbers, so the algorithm can understand them.\n",
    "\n",
    "Our algorithm only knows **certain words.** Run `vectorizer.get_feature_names()` to show you the list of the words it knows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually when we use the vectorizer, we write code like this:\n",
    "    \n",
    "```python\n",
    "vectors = vectorizer.fit_transform(....)\n",
    "```\n",
    "\n",
    "Which both learns all the words **and** counts them. In this case **we already have the list of words we know, we only want to count them.** So instead of `.fit_transform`, we just use `.transform`:\n",
    "\n",
    "```python\n",
    "unknown_vectors = vectorizer.transform(unknown.content)\n",
    "unknown_words_df = ......\n",
    "```\n",
    "\n",
    "Finish making your `unknown_words_df` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm `unknown_words_df` is 11 rows and 2,000 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting with our models\n",
    "\n",
    "To make a prediction for each of our sentences, you can use `.predict` with each of our models. For example, it would look like this for linear regression:\n",
    "\n",
    "```python\n",
    "unknown['pred_linreg'] = linreg.predict(unknown_words_df)\n",
    "```\n",
    "\n",
    "To add the prediction for logistic regression, you'd run similar `.predict` code, which will give you a `0` (negative) or a `1` (positive). A difference between the two is that for logistic regression, you can **also ask for the probability that the sentence is in the `1` category** instead of just simply the category. To do that, you use this code:\n",
    "\n",
    "```python\n",
    "unknown['pred_logreg_prob'] = linreg.predict_proba(unknown_words_df)[:,1]\n",
    "```\n",
    "\n",
    "**Add new columns for each of the models you trained.** If the model has a `.predict_proba`, add that as a column as well. \n",
    "\n",
    "* **Tip:** Tab is helpful for knowing whether `.predict_proba` is an option.\n",
    "* **Tip:** Don't forget the `[:,1]` after `.predict_proba`, it means \"give me the probability for category `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look something like the below. Check your column names to confirm they match up.\n",
    "\n",
    "|content|pred_linreg|pred_logreg|pred_logreg_proba|pred_forest|pred_forest_proba|pred_svc|pred_bayes|pred_bayes_proba|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|I'm not sure how I feel about toast|0.342560|0|0.271403|0|0.5|0|0|0.425271|\n",
    "|...|...|...|...|...|...|...|...|...|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: What do the numbers mean? What's the difference between a 0 and a 1? A 0.5? Negative numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Were there any sentences where the classifiers seemed to disagree about? How do you feel about the amount they disagree? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: What's the difference between using a 0/1 to talk about sentiment compared to 0-1? When might you use one compared to another?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: What's the difference between the linear regression model and the other models we're using? Why might it fit or not fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: Between 0-1, what range do you think counts as \"negative,\" \"positive\" and \"neutral\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9: Does the variation in scores reflect the variation you would see among people? Or is it better or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maybe we should have tested this?\n",
    "\n",
    "We can actually see **which model performs the best**. Let's remind ourselves what we have by looking at:\n",
    "\n",
    "* `X`\n",
    "* `y`\n",
    "* `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our original dataframe is a list of many, many tweets. We turned this into `X` - vectorized words - and `y` - whether the tweet is negative or positive.\n",
    "\n",
    "Before we used `.fit(X, y)` to train on all of our data. Instead, **we can test our models** by doing a test/train split and see if the predictions match the actual labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test and training data \n",
    "\n",
    "Split your `X` and `y` into train and test datasets. I always have to look up how to do it, so here's the code for you:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `X_train` and `y_train` to train all your models, except the linear regression one. You should be training:\n",
    "\n",
    "* `logreg`\n",
    "* `forest`\n",
    "* `svc`\n",
    "* `bayes`\n",
    "\n",
    "Again, do them each in **separate cells** and use `%%time` to see how long each one takes to learn what's a positive vs negative tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrices\n",
    "\n",
    "To see how well they did, we'll use a confusion matrix for each one. For example, here is what you'll use for logistic regression:\n",
    "\n",
    "```python\n",
    "y_true = y_test\n",
    "y_pred = logreg.predict(X_test)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['negative', 'positive'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage-based confusion matrices\n",
    "\n",
    "Those are kind of irritating in that they're just numbers. It might work better if you do something like this instead to get percentages:\n",
    "\n",
    "```python\n",
    "y_true = y_test\n",
    "y_pred = logreg.predict(X_test)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['negative', 'positive'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names) / matrix.sum(axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10: Which models performed the best? Were there big differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11: Do you think it's more important to be sensitive to negativity or positivity? Do we want more positive things incorrectly marked as negative, or more negative things marked as positive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12: They all had very different training times. Which ones offer the best combination of performance and not making you wait around for an hour?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13: If you have a decent algorithm that trains more quickly, that could that mean about feature selection or the size of your training set? Why did we use `max_features=` and `df.sample`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14: How do you feel about sentiment analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15: How do you feel about [this piece from the UpShot](https://www.nytimes.com/interactive/2017/02/28/upshot/trump-sounds-different-tone-in-first-address-to-congress.html) that uses [the Emotional Lexicon](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 16: What would you feel comfortable using our sentiment classifier for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
